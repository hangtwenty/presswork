""" utilities to help do comparisons in tests - esp. given that correct behavior here is (usually) not deterministic
"""
import logging
import re
import string

import presswork.sanitize
from presswork import sanitize
from presswork.sanitize import unicode_dammit
from presswork.text import grammar
from presswork.utils import iter_flatten

logger = logging.getLogger("presswork")


class WordSetComparison(object):
    """ Compares set-of-words in output text (generated) and input text (source text for model)

    Helps to check on a property of (our) markov-chain-text-generators: output words are a subset of input words.

    Refactored to a method-object for pragmatic reasons: w/ py.test, we have this great ability to showlocals
     when something goes wrong, and/or to drop into debugger. more instant gratification if the whole scope of
     this comparison is "there" in the test case scope too. (have confirmed, it is nice to debug with, this way.)

    >>> assert WordSetComparison(["exact"], ["exact"]).output_is_valid_strict()
    >>> assert WordSetComparison([["x", "y", "z"], "nest"], [["x", "y", "z", "abc"], "nest"]).output_is_valid_strict()
    >>> comparison = WordSetComparison(["off", "script"], ["input", "words"])
    >>> assert not comparison.output_is_valid_strict()
    >>> import pytest
    >>> with pytest.raises(ValueError): WordSetComparison("", "").output_is_valid_strict()
    """

    def __init__(self, generated_tokens, input_tokenized):
        """
        :param generated_tokens: words generated by make_sentences(). it can be nested; will flatten if needed.
        :param input_tokenized: words tokenized from input text. it can be nested; will flatten if needed.

        note, tokenization must be same on 'output' and 'input' - compare like with like.
        """
        if isinstance(generated_tokens, basestring) or isinstance(input_tokenized, basestring):
            raise ValueError('please only pass already-tokenized items for comparison.')

        self._generated_tokens = list(iter_flatten(generated_tokens))
        self._input_tokenized = list(iter_flatten(input_tokenized))

        self.set_of_generated_words = set(self._denoise_words(self._generated_tokens))
        self.set_of_input_words = set(self._denoise_words(self._input_tokenized))

        # avoid comparing empty set(s). (why: set().issubsetof({1,2,3}) == True, but that doesn't mean much for us.)
        if not self.set_of_generated_words or not self.set_of_input_words:
            raise ValueError("this test is not meaningful if either word-set is empty. check for other errors!")

        self._difference = None

    def output_is_valid_strict(self):
        """ A property of (our) markov-chain-text-generators: output words are a subset of input words. Check this.
        """
        is_subset = self.set_of_generated_words.issubset(self.set_of_input_words)
        return is_subset

    @property
    def difference(self):
        if self._difference is None:
            self._difference = self.set_of_generated_words.difference(self.set_of_input_words)
        return self._difference

    def _denoise_words(self, words):
        """ strip away noise from the words, i.e. remove punctuation
        """
        return filter(None, (denoise_punctuation(word) for word in words))


class FrontendWordSetComparison(WordSetComparison):
    """ for use in testing CLI and Flask app, where Joiners have already been called before output.

    why different: the Joiners are free to do things with punctuation and spacing, that can complicate "re-tokenizing"
     before compare. in order to keep the direct text maker tests tighter, but still be robust in CLI & Flaskapp tests,
     this variant is just a bit more brutish.
    """

    def __init__(self, generated_tokens, input_tokenized, _generated_text=None, _input_text=None):
        super(FrontendWordSetComparison, self).__init__(
                generated_tokens=generated_tokens, input_tokenized=input_tokenized)
        self._generated_text = _generated_text
        self._input_text = _input_text

    @classmethod
    def create(cls, generated_text, input_text, tokenizer):
        """ takes in raw text, filters some (destroy punctuation), then constructs WordSetComparison.
        """
        _cleaned_generated_text = cls._clean_text(generated_text)
        _cleaned_input_text = cls._clean_text(input_text)

        instance = cls(
                generated_tokens=tokenizer.tokenize(_cleaned_generated_text),
                input_tokenized=tokenizer.tokenize(_cleaned_input_text),
                _generated_text=generated_text,
                _input_text=input_text,
        )
        return instance

    @staticmethod
    def _clean_text(text):
        return denoise_punctuation(
                sanitize.OutputProofreader().proofread(
                        sanitize.SanitizedString(text).unwrap()
                )
        )

    def phantom_tokens(self):
        """ confirm all output tokens are *somewhere* in input text

        it's pretty course - if you used this alone, you'd give yourself a lot of false-positive passes.
        however as an additional check, together with output_is_mostly_valid, it helps confirm there isn't, somehow,
        stuff being output that has no connection to the input.

        in other words, it WOULD fail in some totally bogus scenarios, so it is a good *additional* check.
        """
        phantoms = []
        cleaned_input = self._clean_text(self._input_text)
        for token in self._generated_tokens:
            if token not in cleaned_input:
                phantoms.append(token)
                logger.warning("phantom token? {!r}".format(token))
        return phantoms

    def output_is_mostly_valid(self, tolerance):
        """ in the Frontend cases, we allow a little difference, on hope it only has to do with "display/joiner" quirks

        (I held out for a while trying to ensure 100% validity even in front end, but got diminishing returns as
        I kept trying to make that work. this seemed like an OK compromise for now.)

        :param tolerance: percentage expressed as 0-1.0 (meaning 0-100%). higher is more tolerant of difference.
            this shouldn't be more than 0.01 i.e. 1%, very often.
        """
        if tolerance > 0.012:
            raise ValueError("tolerance = {} i.e. {}%, aborting, try to fix underlying issues first".format(
                    tolerance, tolerance * 100.0))
        strictly_valid = self.output_is_valid_strict()
        if strictly_valid:
            return True
        else:
            difference_score = float(len(self.difference)) / len(self.set_of_generated_words)
            logger.debug("difference score = {}".format(difference_score))
            if difference_score <= tolerance:
                mostly_valid = True
            else:
                mostly_valid = False

        if self.phantom_tokens():
            mostly_valid = False

        return mostly_valid



re_ascii_punctuation = re.compile(u'[%s]' % re.escape(string.punctuation), flags=re.UNICODE)

def denoise_punctuation(s):
    """ try to reduce noise and false failures, by messing with punctuation before (re-)tokenizing for comparison

    This method has some black magic from trial and error with real test cases.

        >>> import string
        >>> assert denoise_punctuation(string.punctuation).strip() == ""
        >>> print denoise_punctuation(string.punctuation + " hello " + string.punctuation).strip()
        hello
        >>> print denoise_punctuation(string.punctuation + " can't " + string.punctuation).strip()
        cant
        >>> newline = chr(10)  # next case is based on a real bug
        >>> print denoise_punctuation("More of the Same." + newline + "To amend")
        More of the Same
        To amend
    """
    # TODO in a followup commmit, delete these sketches that I ultimately discarded
    # re_ascii_punctuation = re.compile(u'(?<!\w)[%s](?!\w)' % re.escape(string.punctuation), flags=re.UNICODE)
    # re_ascii_punctuation = re.compile(u'[\w\s]([%s])[\w\s]' % re.escape(string.punctuation), flags=re.UNICODE)

    # #
    # # def _keep_single_apostrophe_or_quote(match):
    # #     punct_string = match.group(2)
    # #     if match.group(3) and match.group(3)[0].isupper():
    # #         # from the wild: Edward'Ted
    # #         return u" "
    # #     elif punct_string in (u"'", u'"'):
    # #         return match.group(0).strip('\'"')
    # #     else:
    # #         return u""
    # #
    # # # return re_ascii_punctuation.sub(_keep_apostrophe_and_quotes_but_else_replace_with_space, cleaned)
    # # # return re_ascii_punctuation.sub("<PUNCTUATION>", cleaned)
    # # return re_ascii_punctuation.sub(_keep_single_apostrophe_or_quote, cleaned)
    #
    # re_ascii_punctuation = re.compile(u'[\w\s]([%s])[\w\s]' % re.escape(string.punctuation), flags=re.UNICODE)
    # def _func(match):
    #     punct = match.group(1)
    #     all = match.group(0)
    #     return all.replace(punct, u"")
    #
    # return re_ascii_punctuation.sub(_func, s)

    return re_ascii_punctuation.sub("", s)


